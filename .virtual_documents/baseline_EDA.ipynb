


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_percentage_error


cdf = pd.read_csv('cleaned_ameshousing.csv')


pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)


cdf.head()





    try:
        with open('/home/charles/Documents/practice/coding_temple_repo/My-M7-Project-Home-Price-Predictions/data/DataDocumentation.txt', 'r', encoding='utf-8') as f:
            content = f.read()
            print(content)
    except UnicodeDecodeError:
        print("UTF-8 encoding failed. Trying 'latin-1'...")
        with open('/home/charles/Documents/practice/coding_temple_repo/My-M7-Project-Home-Price-Predictions/data/DataDocumentation.txt', 'r', encoding='latin-1') as f:
            content = f.read()
            print(content)
    except Exception as e:
        print(f"An error occurred: {e}")


['','','','','','','','']


plt.figure(figsize=(6, 25))
sns.heatmap(cdf.corr()[['SalePrice']].sort_values(by = 'SalePrice', ascending = False), 
           vmin= -1, 
           vmax= 1, 
           annot = True, 
           cmap = 'coolwarm')


sub = cdf[['SalePrice','Overall Qual','Exter Qual','Gr Liv Area','Kitchen Qual','Garage Cars','Garage Area','Total Bsmt SF']]


sub.to_csv('baseline_ameshousing.csv', index = False)





df = pd.read_csv('baseline_ameshousing.csv')


df['Overall Qual'].value_counts()


df['Exter Qual'].value_counts()


df['Gr Liv Area'].value_counts()


df['Overall Qual'].value_counts()


df['Overall Qual'].value_counts()


df['Overall Qual'].value_counts()


df.head()


df.shape





plt.figure(figsize=(6, 8))
sns.heatmap(df.corr()[['SalePrice']].sort_values(by = 'SalePrice', ascending = False), 
           vmin= -1, 
           vmax= 1, 
           annot = True, 
           cmap = 'coolwarm')


sns.set_style('darkgrid')


sns.pairplot(df, corner = True, 
            x_vars= 'SalePrice')


fig, ax = plt.subplots(figsize = (7, 5))
sns.barplot(df, x = 'Exter Qual', y = 'SalePrice', estimator='median', errorbar =('pi', 50))
ax.bar_label(ax.containers[0], label_type='center')


fig, ax = plt.subplots(figsize = (7, 5))
sns.barplot(df, x = 'Garage Cars', y = 'SalePrice', estimator='median', errorbar =('pi', 50))
ax.bar_label(ax.containers[0], label_type='center')


fig, ax = plt.subplots(figsize = (7, 5))
sns.barplot(df, x = 'Kitchen Qual', y = 'SalePrice', estimator='median', errorbar =('pi', 50))
ax.bar_label(ax.containers[0], label_type='center')


fig, ax = plt.subplots(figsize = (7, 5))
sns.barplot(df, x= 'Overall Qual', y='SalePrice', estimator='median', errorbar =('pi', 50))
ax.bar_label(ax.containers[0], label_type='center')














X = df.drop(columns = 'SalePrice')

y = df['SalePrice']


scores = []

for i in range(20, 31):
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = i/100)
    lr = LinearRegression()
    lr.fit(X_train, y_train)
    lr.predict(X_test)
    train_score = lr.score(X_train, y_train)
    test_score = lr.score(X_test, y_test)
    scores.append({'i' : i, 'train_score' : train_score, 'test_score' : test_score})
lr_scores = pd.DataFrame(scores)


lr_scores
# 0.26 test size


scores = []

for i in range(20, 31):
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = i/100)
    rfr = RandomForestRegressor()
    rfr.fit(X_train, y_train)
    rfr.predict(X_test)
    train_score = rfr.score(X_train, y_train)
    test_score = rfr.score(X_test, y_test)
    scores.append({'i' : i, 'train_score' : train_score, 'test_score' : test_score})
rfr_scores = pd.DataFrame(scores)


rfr_scores
# test size 23


scores = []

for i in range(20, 31):
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = i/100)
    dtr = DecisionTreeRegressor()
    dtr.fit(X_train, y_train)
    dtr.predict(X_test)
    train_score = dtr.score(X_train, y_train)
    test_score = dtr.score(X_test, y_test)
    scores.append({'i' : i, 'train_score' : train_score, 'test_score' : test_score})
dtr_scores = pd.DataFrame(scores)


dtr_scores
# test size 28


scores = []

for i in range(20, 31):
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = i/100)
    knn = KNeighborsRegressor()
    knn.fit(X_train, y_train)
    knn.predict(X_test)
    train_score = knn.score(X_train, y_train)
    test_score = knn.score(X_test, y_test)
    scores.append({'i' : i, 'train_score' : train_score, 'test_score' : test_score})
knn_scores = pd.DataFrame(scores)


knn_scores
# test size 24








X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.26)

lr = LinearRegression()

lr.fit(X_train, y_train)


lr_preds = lr.predict(X_test)
baseline_preds = np.full_like(y_test, y_test.mean())


lr.score(X_test, y_test)
# 0.800931


lr_model_rmse = np.sqrt(mean_squared_error(y_test, lr_preds))
print(lr_model_rmse)


baseline_rmse = np.sqrt(mean_squared_error(y_test, baseline_preds))
print(baseline_rmse)


sale_mean = y_train.mean()
(lr_model_rmse / sale_mean) * 100
# NRMSE 
# RMSE score, but scaled. shows the average error of a model (my predictions are off on average by about 21.3 percent every time it makes a prediction)


mean_absolute_percentage_error(y_test, lr_preds)
# shows how far off predictions are on average (Average Magnitude of error) between the predicted and actual values
# MAE but as a percentage/scaled
# predicted values are off from their actual values by about 14.6 percent


mean_absolute_percentage_error(y_test, baseline_preds)








pkl_file = ''


with open('pkl_file', 'wb') as file:
    model = pickle.dump(lr, file)


with open('pkl_file', 'rb') as file:
    model = pickle.load(file)





X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.23)

rfr = RandomForestRegressor()

rfr.fit(X_train, y_train)


rfr_preds = rfr.predict(X_test)
baseline_preds = np.full_like(y_test, y_test.mean())


rfr.score(X_test, y_test)
# 0.855978
# omg, pretty consistent for a random forest regressor


rfr_model_rmse = np.sqrt(mean_squared_error(y_test, rfr_preds))
print(rfr_model_rmse)


np.sqrt(mean_squared_error(y_test, baseline_preds))


sale_mean = y_train.mean()
(rfr_model_rmse / sale_mean) * 100


mean_absolute_percentage_error(y_test, rfr_preds)


mean_absolute_percentage_error(y_test, baseline_preds)





X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.28)

dtr = DecisionTreeRegressor()

dtr.fit(X_train, y_train)


dtr_preds = dtr.predict(X_test)
baseline_preds = np.full_like(y_test, y_test.mean())


dtr.score(X_test, y_test)
# 0.800105
# question: what if there are big fluctuations within my dtr model?
# Because it is likely overfitted!
# 
# I think this might be my worst model now


dtr_model_rmse = np.sqrt(mean_squared_error(y_test, dtr_preds))
print(dtr_model_rmse)


np.sqrt(mean_squared_error(y_test, baseline_preds))


sale_mean = y_train.mean()
(dtr_model_rmse / sale_mean) * 100


mean_absolute_percentage_error(y_test, dtr_preds)


mean_absolute_percentage_error(y_test, baseline_preds)





X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.24)

knn = KNeighborsRegressor()

knn.fit(X_train, y_train)


knn_preds = knn.predict(X_test)
baseline_preds = np.full_like(y_test, y_test.mean())


knn.score(X_test, y_test)
# 0.790094


knn_model_rmse = np.sqrt(mean_squared_error(y_test, knn_preds))
print(knn_model_rmse)


np.sqrt(mean_squared_error(y_test, baseline_preds))


sale_mean = y_train.mean()
(knn_model_rmse / sale_mean) * 100


mean_absolute_percentage_error(y_test, knn_preds)


mean_absolute_percentage_error(y_test, baseline_preds)












